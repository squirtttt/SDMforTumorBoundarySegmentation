{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fdfee7e2",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2129070b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#1\n",
    "import os\n",
    "import sys\n",
    "# os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "sys.path.append(\"./\")\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random, tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import albumentations as album\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "#from dataloaders.datasets import Pathology\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# sdm\n",
    "from scipy.ndimage import distance_transform_edt as distance\n",
    "from skimage import segmentation as skimage_seg\n",
    "from skimage import morphology\n",
    "\n",
    "# test loader\n",
    "import itertools\n",
    "\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "\n",
    "import math\n",
    "\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afa392b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "class Path(object):\n",
    "    @staticmethod\n",
    "    def pathology_root_dir():\n",
    "        root_dir = \"\"\n",
    "        return os.path.join(root_dir, \"BCSS_patch\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e6818d3",
   "metadata": {},
   "source": [
    "### data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee8fa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "#helper function for data visualization\n",
    "def visualize(**images):\n",
    "    \"\"\"\n",
    "    Plot images in one row\n",
    "    \"\"\"\n",
    "    n_images = len(images)\n",
    "    plt.figure(figsize=(20,8))\n",
    "    for idx, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n_images, idx + 1)\n",
    "        plt.xticks([]); \n",
    "        plt.yticks([])\n",
    "        # get title from the parameter names\n",
    "        plt.title(name.replace('_',' ').title(), fontsize=20)\n",
    "        plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "# Perform one hot encoding on label\n",
    "def one_hot_encode(label, label_values):\n",
    "    semantic_map = []\n",
    "    for colour in label_values:\n",
    "        equality = np.equal(label, colour)\n",
    "        class_map = np.all(equality, axis = -1)\n",
    "        semantic_map.append(class_map)\n",
    "    semantic_map = np.stack(semantic_map, axis=-1)\n",
    "\n",
    "    return semantic_map\n",
    "    \n",
    "# Perform reverse one-hot-encoding on labels / preds\n",
    "def reverse_one_hot(image):\n",
    "    x = np.argmax(image, axis = -1)\n",
    "    return x\n",
    "\n",
    "# Perform colour coding on the reverse-one-hot outputs\n",
    "def colour_code_segmentation(image, label_values):\n",
    "    colour_codes = np.array(label_values)\n",
    "    x = colour_codes[image.astype(int)]\n",
    "\n",
    "    return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ef28dd9",
   "metadata": {},
   "source": [
    "### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5c3f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "# Useful to shortlist specific classes in datasets with large number of classes\n",
    "class_names = ['background', 'tumor ', 'stroma', 'dcis']\n",
    "select_classes = ['background', 'tumor ', 'stroma', 'dcis']\n",
    "\n",
    "background=[[0, 0, 0],]\n",
    "\n",
    "# Get RGB values of required classes\n",
    "class_rgb_values = [[0, 0, 0], [255, 0, 0], [0, 255, 0], [0, 0, 255]]\n",
    "\n",
    "select_class_indices = [class_names.index(cls.lower()) for cls in select_classes]\n",
    "select_class_rgb_values =  np.array(class_rgb_values)[select_class_indices]\n",
    "\n",
    "print('Selected classes and their corresponding RGB values in labels:')\n",
    "print('Class Names: ', class_names)\n",
    "print('Class RGB values: ', class_rgb_values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e999a997",
   "metadata": {},
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855d04ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5\n",
    "def get_training_augmentation():\n",
    "    train_transform = [    \n",
    "        #album.RandomCrop(height=256, width=256, always_apply=True),\n",
    "        album.OneOf(\n",
    "            [\n",
    "                album.HorizontalFlip(p=1),\n",
    "                album.VerticalFlip(p=1),\n",
    "                album.RandomRotate90(p=1),\n",
    "            ],\n",
    "            p=1,\n",
    "        ),\n",
    "    ]\n",
    "    return album.Compose(train_transform)\n",
    "\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "\n",
    "def get_preprocessing(preprocessing_fn=None):\n",
    "    _transform = []\n",
    "    if preprocessing_fn:\n",
    "        _transform.append(album.Lambda(image=preprocessing_fn))\n",
    "    _transform.append(album.Lambda(image=to_tensor, mask=to_tensor))\n",
    "        \n",
    "    return album.Compose(_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5612797",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabeledDataset(Dataset):\n",
    "    def __init__(self, args, augmentation, split:str, img_list=None,  fraction: float=1.0, seed: int=42):\n",
    "        self.augmentation = augmentation\n",
    "        self.args = args\n",
    "        self.data_dir = os.path.join(Path.pathology_root_dir(), 'labelled', split, 'png')\n",
    "\n",
    "        if img_list is not None:\n",
    "            self.imgs = img_list\n",
    "            \n",
    "        else:\n",
    "            all_imgs = sorted([os.path.join(self.data_dir, f) for f in os.listdir(self.data_dir)])\n",
    "            if fraction <= 1.0:\n",
    "                np.random.seed(seed)\n",
    "                selected_indices = np.random.choice(len(all_imgs), int(len(all_imgs)*fraction), replace=False)\n",
    "                self.imgs = [all_imgs[i] for i in sorted(selected_indices)]\n",
    "            else: \n",
    "                self.imgs = all_imgs\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        _img = cv2.cvtColor(cv2.imread(self.imgs[index]), cv2.COLOR_BGR2RGB)\n",
    "        mask_path = self.imgs[index].replace(\"png\", \"npy\")\n",
    "        _label = np.load(mask_path, allow_pickle=True).item().get('label')\n",
    "\n",
    "        if self.augmentation != None:\n",
    "            sample = self.augmentation(image=_img, mask=_label)\n",
    "            _img, _label = sample['image'], sample['mask']\n",
    "        \n",
    "        _img = _img/255\n",
    "        transform = A.Compose([ToTensorV2()])\n",
    "        _img = transform(image=_img)\n",
    "        _label = torch.as_tensor(_label).long()\n",
    "\n",
    "        sample = {\n",
    "            'img': _img,\n",
    "            'label':_label,\n",
    "            'path': mask_path\n",
    "        }\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b518f4d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_aug_dataset = LabeledDataset([], get_training_augmentation(), \"train\", None, 0.5)\n",
    "\n",
    "labeled_img_list = train_aug_dataset.imgs\n",
    "print(len(train_aug_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60de2faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):   \n",
    "    batch = [b for b in batch if b is not None]\n",
    "    return torch.utils.data.dataloader.default_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b2c313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate unlabeled dataset\n",
    "class UnlabeledDataset(Dataset):\n",
    "    def __init__(self, args, augmentation, split: str, img_list):\n",
    "        self.augmentation = augmentation\n",
    "        self.args = args\n",
    "        self.data_dir = os.path.join(Path.pathology_root_dir(), 'labelled', split, 'png')\n",
    "\n",
    "        exclude_files = set([os.path.basename(path) for path in img_list])\n",
    "\n",
    "        all_imgs = sorted(os.listdir(self.data_dir))\n",
    "        remaining_imgs = [f for f in all_imgs if f not in exclude_files]\n",
    "\n",
    "        self.imgs = [os.path.join(self.data_dir, f) for f in remaining_imgs]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        _img = cv2.cvtColor(cv2.imread(self.imgs[index]), cv2.COLOR_BGR2RGB)\n",
    "        mask_path = self.imgs[index]\n",
    "\n",
    "        _img = _img / 255\n",
    "        transform = A.Compose([ToTensorV2()])\n",
    "        _img = transform(image=_img)\n",
    "\n",
    "        sample = {\n",
    "            'img': _img,\n",
    "            'path': mask_path\n",
    "        }\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b6a960",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generate train_loader\n",
    "\n",
    "def labeled_make_loaders(args, num_workers, pin_memory=True):\n",
    "    train_set = LabeledDataset([], None, \"train\", labeled_img_list)+train_aug_dataset\n",
    "    test_set = LabeledDataset([], None, \"test\")\n",
    "    unlabel_set = UnlabeledDataset([], None, \"train\", labeled_img_list)\n",
    "\n",
    "    # split validation set 8:2\n",
    "    # 4598*2 \n",
    "    train_set, val_set = torch.utils.data.random_split(train_set, [7356, 1840])\n",
    "\n",
    "    print(f\"train set: {len(train_set)} | val set: {len(val_set)} | test set: {len(test_set)} | unlabel set: {len(unlabel_set)}\")\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_set, batch_size=16, shuffle = True, \n",
    "        num_workers=8, pin_memory=True,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_set, batch_size = 16, shuffle=True,\n",
    "        num_workers=8, pin_memory=True,\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_set, shuffle=False, num_workers=10, pin_memory=True\n",
    "    )\n",
    "\n",
    "    unlabel_loader = DataLoader(\n",
    "        unlabel_set, shuffle=False, num_workers=10, pin_memory=True\n",
    "    )\n",
    "\n",
    "\n",
    "    return train_loader, val_loader, test_loader, unlabel_loader\n",
    "\n",
    "label_train_loader, label_val_loader, test_loader, unlabel_loader = labeled_make_loaders([], num_workers=8, pin_memory=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0a9eb30",
   "metadata": {},
   "source": [
    "### Parameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa5df5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9\n",
    "#label smoothing\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.0, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        assert 0 <= self.smoothing < 1\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ff96b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10\n",
    "def accuracy(dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for data in dataloader:\n",
    "            inputs = data['img']['image']\n",
    "            labels = data['label']\n",
    "            images, labels = inputs.float().to(DEVICE), labels.to(DEVICE, dtype=torch.int64)\n",
    "            outputs = model(images)\n",
    "            #outputs = torch.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(outputs.detach(), 1)\n",
    "            total += labels.size(0)      \n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    acc = 100*correct/total\n",
    "    model.train()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1729c85",
   "metadata": {},
   "source": [
    "### model configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a253e01",
   "metadata": {},
   "source": [
    "model listup\n",
    "- semi-supervised\n",
    "    - Unet\n",
    "        - encoder: resnet50\n",
    "        - encoder-weights: none\n",
    "    - Unet\n",
    "        - encoder: timm-resnest50d\n",
    "    - Deeplabv3+\n",
    "        - encoder: resnet50\n",
    "    - Segformer\n",
    "        - mitb2\n",
    "    - Unet : with sdm\n",
    "        - encoder: resnet50\n",
    "        - encoder-weights: imagenet\n",
    "        - activation: identity\n",
    "    - Unet : with sdm\n",
    "        - encoder: resnest50\n",
    "        - encoder-weights: imagenet\n",
    "        - activation: identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fbc009",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#11\n",
    "import segmentation_models_pytorch as smp\n",
    "ENCODER = 'timm-resnest50d' #'timm-resnest50d' 'densenet201' 'resnet34'\n",
    "ENCODER_WEIGHTS = None\n",
    "CLASSES = class_names\n",
    "ACTIVATION ='softmax2d'  #'identity' # 'softmax2d'\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#if torch.cuda.is_available():\n",
    "#    torch.cuda.set_device(DEVICE)\n",
    "print(DEVICE)\n",
    "#ACTIVATION = 'sigmoid'could be None for logits or 'softmax2d' for multiclass segmentation\n",
    "\n",
    "# create segmentation model with pretrained encoder\n",
    "model = smp.Unet(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS, \n",
    "    classes=len(CLASSES), \n",
    "    activation= ACTIVATION,\n",
    ")\n",
    "\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228e8558",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#12\n",
    "# Set flag to train the model or not. If set to 'False', only prediction is performed (using an older model checkpoint)\n",
    "TRAINING = True\n",
    "\n",
    "# Set num of epochs\n",
    "#update2 =70 \n",
    "iterations = 80\n",
    "\n",
    "\n",
    "\n",
    "print(\"Device : \",DEVICE)\n",
    "\n",
    "# define loss function\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "#(3, 0.4)\n",
    "criterion = LabelSmoothingLoss(3, 0.2)\n",
    "\n",
    "optimizer = torch.optim.Adam([ \n",
    "    dict(params=model.parameters(), lr=0.0001, betas=(0.92, 0.99)),\n",
    "])\n",
    "\n",
    "#update2 =7 or 5\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=7, eta_min=0.0001,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b90369d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#13\n",
    "import time\n",
    "trainLoss = []\n",
    "valLoss = []\n",
    "#valDice = []\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "alpha = 0\n",
    "beta = 0.3\n",
    "alpha_t = 1e-4\n",
    "T1 = 5\n",
    "T2 = 70\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1848c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sdm map calculation\n",
    "def compute_sdf(img_gt, out_shape):\n",
    "    \"\"\"\n",
    "    compute the signed distance map of binary mask\n",
    "    input: segmentation, shape = (batch_size, c, x, y)\n",
    "    output: the Signed Distance Map (SDM) \n",
    "    sdf(x) = 0; x in segmentation boundary\n",
    "             -inf|x-y|; x in segmentation\n",
    "             +inf|x-y|; x out of segmentation\n",
    "    normalize sdf to [-1,1]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    img_gt = img_gt.astype(np.uint8)\n",
    "    normalized_sdf = np.zeros(out_shape)\n",
    "    # thresh = 15\n",
    "\n",
    "    for b in range(out_shape[0]):  # batch size\n",
    "    # Foreground mask: cls1 or cls2\n",
    "        posmask = np.isin(img_gt[b], [1, 2]).astype(bool)  # cls1과 cls2를 foreground로 설정\n",
    "        # posmask = np.isin(img_gt[b], [1]).astype(bool)\n",
    "        if posmask.any():\n",
    "        \n",
    "            negmask = ~posmask\n",
    "            posdis = distance(posmask)\n",
    "            negdis = distance(negmask)\n",
    "            # Apply distance function and log scaling\n",
    "            # posdis = np.log1p(distance(posmask))  # Log transformation to compress range\n",
    "            # negdis = np.log1p(distance(negmask))  # Log transformation for background\n",
    "            \n",
    "            boundary = skimage_seg.find_boundaries(posmask, mode='inner').astype(np.uint8)\n",
    "            # print(f\"min negdis: {np.min(negdis)}, max negdis: {np.max(negdis)}, min posdis: {np.min(posdis)}, max posdis: {np.max(posdis)}\")\n",
    "            # Signed Distance Map 계산\n",
    "            max_posdis = np.max(posdis)\n",
    "            max_negdis = np.max(negdis)\n",
    "            if max_posdis == 0 or max_negdis == 0:\n",
    "                sdf = np.zeros_like(posdis)\n",
    "            else:\n",
    "                sdf = (negdis - np.min(negdis)) / (np.max(negdis) - np.min(negdis)) - (posdis - np.min(posdis)) / (np.max(posdis) - np.min(posdis))\n",
    "                sdf[boundary == 1] = 0\n",
    "            # print(f\"max sdf: {np.max(sdf)}, min sdf: {np.min(sdf)}\")\n",
    "            normalized_sdf[b] = sdf  # batch 내에서 전체 foreground에 대한 SDM 저장\n",
    "\n",
    "\n",
    "    return normalized_sdf\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2446f757",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43908ecf",
   "metadata": {},
   "source": [
    "### pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19ea1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#14: semi-supervised learning: pre-training\n",
    "torch.backends.cudnn.benchmark = False\n",
    "print(f\"pretraining starts!!\")\n",
    "for epoch in range(pre_iterations):\n",
    "    print(f\"{epoch} iteration start!\")\n",
    "    best_score = 999999\n",
    "    epochStart = time.time()\n",
    "    runningLoss = 0\n",
    "    train_i_number = 0\n",
    "    val_i_number = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    best_acc = 0\n",
    "    model.train() # For training\n",
    "    print(f'train loader: {len(label_train_loader)}')\n",
    "    for traindata in tqdm.notebook.tqdm(label_train_loader):\n",
    "        train_inputs = traindata['img']['image']\n",
    "        train_labels = traindata['label']\n",
    "\n",
    "        train_inputs, train_labels = train_inputs.float().to(DEVICE), train_labels.to(device=DEVICE, dtype=torch.int64)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred = model(train_inputs)\n",
    "        train_outputs = F.softmax(pred, dim=1)\n",
    "\n",
    "        # compute sdm\n",
    "        pred_sdm = torch.tanh(pred)\n",
    "\n",
    "        label_sdm = torch.from_numpy(compute_sdm(train_labels)).float().to(pred_sdm.device)\n",
    "        # print(f'pred shape: {pred_sdm.shape}, label shape: {label_sdm.shape}')\n",
    "        sdm_loss = F.mse_loss(pred_sdm, label_sdm)\n",
    "    \n",
    "        # compute loss\n",
    "        loss = criterion(train_outputs, train_labels) + (beta*sdm_loss)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predicted = torch.max(train_outputs.detach(), 1)\n",
    "        total += train_labels.size(0)\n",
    "        correct += (predicted == train_labels).sum().item()\n",
    "    val_acc = accuracy(label_val_loader)\n",
    "\n",
    "    if val_acc >= best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save(model.state_dict(), './pretrained/sdm_unet_resnest50.pth')\n",
    "        print('[%d] train acc: %.2f, validation acc: %.2f - Saved the best model' %(epoch, 100*correct/total, val_acc))\n",
    "        print(\"*******\") \n",
    "    elif epoch % 10 == 0:\n",
    "        print('[%d] train acc: %.2f, validation acc: %.2f' %(epoch, 100*correct/total, val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54a92cf",
   "metadata": {},
   "source": [
    "### save pseudo-labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097424c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save pseudo-labels\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm.notebook.tqdm(unlabel_loader):\n",
    "        unlabel_input = batch['img']['image']\n",
    "        unlabel_input = unlabel_input.float().to(DEVICE)\n",
    "        path = os.path.basename(batch['path'][0]).replace('png', 'npy')\n",
    "\n",
    "        unlabel_output = model(unlabel_input) \n",
    "        \n",
    "        unlabel_output = F.softmax(unlabel_output, dim=1)\n",
    "        _, pred_mask = torch.max(unlabel_output, 1)\n",
    "        pred_mask = pred_mask[0].cpu().numpy()\n",
    "        image_np = unlabel_input.cpu().numpy()\n",
    "\n",
    "        save_dict= {\n",
    "            'input': image_np, \n",
    "            'label': pred_mask\n",
    "        }\n",
    "\n",
    "        save_path = os.path.join(Path.pathology_root_dir(), 'pseudo', 'sdm_unet_resnest50', 'npy')\n",
    "        os.makedirs(save_path, exist_ok=True) \n",
    "        save_file = os.path.join(save_path, path)\n",
    "        np.save(save_file, save_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a7e922",
   "metadata": {},
   "source": [
    "### pseudo-label dataset load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c7d0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PseudoDataset(Dataset):\n",
    "    def __init__(self, args, augmentation, split:str, img_list):\n",
    "        \n",
    "        self.augmentation = augmentation\n",
    "        self.args = args\n",
    "        self.img_dir = os.path.join(Path.pathology_root_dir(), 'labelled', split, 'png')\n",
    "        \n",
    "        exclude_files = set([os.path.basename(p) for p in img_list])\n",
    "\n",
    "        all_imgs = sorted(os.listdir(self.img_dir))\n",
    "\n",
    "        \n",
    "        remaining_imgs = [f for f in all_imgs if f not in exclude_files]\n",
    "\n",
    "        self.imgs = [os.path.join(self.img_dir, f) for f in remaining_imgs]\n",
    "    \n",
    "        \n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        _img = cv2.cvtColor(cv2.imread(self.imgs[index]), cv2.COLOR_BGR2RGB)\n",
    "        file_name = os.path.basename(self.imgs[index]).replace(\"png\", \"npy\")\n",
    "        label_path = os.path.join(Path.pathology_root_dir(), 'pseudo','sdm_unet_resnest50', 'npy', file_name)\n",
    "        _label = np.load(label_path, allow_pickle=True).item().get('label')\n",
    "\n",
    "        if self.augmentation != None:\n",
    "            sample = self.augmentation(image= _img, mask = _label)\n",
    "            _img, _label = sample['image'], sample['mask']\n",
    "\n",
    "        _img = _img/255\n",
    "        transform = A.Compose([ToTensorV2()])\n",
    "        _img = transform(image=_img)\n",
    "        _label = torch.as_tensor(_label).long()\n",
    "\n",
    "        sample = {\n",
    "            'img': _img,\n",
    "            'label': _label, \n",
    "            'path': label_path\n",
    "        }\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88398967",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_aug_dataset = PseudoDataset([], get_training_augmentation(), \"train\", labeled_img_list)\n",
    "\n",
    "print(len(pseudo_aug_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce541de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aug_dataset = LabeledDataset([], get_training_augmentation(), \"train\", labeled_img_list, 0.5)\n",
    "\n",
    "print(len(train_aug_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1419331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labeled+pseudo data loader\n",
    "def whole_make_loaders(args, num_workers, pin_memory=True):\n",
    "    pseudo_set = PseudoDataset([], None, \"train\", labeled_img_list) +pseudo_aug_dataset\n",
    "    label_set = LabeledDataset([], None, \"train\", labeled_img_list) +train_aug_dataset\n",
    "    \n",
    "\n",
    "    test_set = LabeledDataset([], None, \"test\")\n",
    "\n",
    "    total_len = len(label_set)\n",
    "    train_len = int(total_len*0.8)\n",
    "    val_len = total_len - train_len\n",
    "\n",
    "    pseudo_len = len(pseudo_set)\n",
    "    pseudo_train_len = int(pseudo_len*0.8)\n",
    "    pseudo_val_len = pseudo_len - pseudo_train_len\n",
    "\n",
    "    labeled_train_set, labeled_val_set = torch.utils.data.random_split(label_set, [train_len, val_len])\n",
    "    pseudo_train_set, pseudo_val_set = torch.utils.data.random_split(pseudo_set, [pseudo_train_len, pseudo_val_len])\n",
    "\n",
    "    val_set = pseudo_val_set + labeled_val_set\n",
    "\n",
    "    print(f'labeled train set: {len(labeled_train_set)} | pseudo train set: {len(pseudo_train_set)} | val set: {len(val_set)} | test_set: {len(test_set)}')\n",
    "\n",
    "    # print(f'label- train: {train_len}, val: {val_len}')\n",
    "    # print(f'pseudo- train: {pseudo_train_len}, val: {pseudo_val_len}')\n",
    "    train_loader = DataLoader(\n",
    "        labeled_train_set, batch_size=16, shuffle=True,\n",
    "        num_workers=8, pin_memory=True,\n",
    "    )\n",
    "\n",
    "    pseudo_train_loader = DataLoader(\n",
    "        pseudo_train_set, batch_size=16, shuffle=True,\n",
    "        num_workers=8, pin_memory=True,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_set, batch_size=16, shuffle=False,\n",
    "        num_workers=8, pin_memory=True,\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_set, shuffle=False, num_workers=10, pin_memory=True,\n",
    "    )\n",
    "    \n",
    "    return train_loader, pseudo_train_loader, val_loader, test_loader\n",
    "\n",
    "train_loader, pseudo_train_loader, val_loader, test_loader = whole_make_loaders([], num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d439185a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label+pseudo train : without sdm\n",
    "torch.cuda.empty_cache()\n",
    "torch.backends.cudnn.benchmark = False\n",
    "for epoch in range(iterations):\n",
    "    print(epoch,\"epoch start !\")\n",
    "    best_score=999999\n",
    "    epochStart = time.time()\n",
    "    runningLoss = 0\n",
    "    train_i_number = 0\n",
    "    val_i_number = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    best_acc = 0\n",
    "    model.train() # For training\n",
    "    print(\"train_loader\", len(train_loader))\n",
    "    # for traindata in zip(train_loader):\n",
    "    for traindata, pseudodata in zip(train_loader, pseudo_train_loader):\n",
    "        #print(f\"train data {i+1}/{len(train_loader)}\")\n",
    "        #print(\"*\")\n",
    "        train_inputs = traindata['img']['image']\n",
    "        train_labels = traindata['label']\n",
    "\n",
    "        pseudo_inputs = pseudodata['img']['image']\n",
    "        pseudo_labels = pseudodata['label']\n",
    "        #print(\"train_label : \", type(train_inputs), \"pseudo label : \", type(pseudo_inputs))\n",
    "        \n",
    "        train_inputs, train_labels = train_inputs.float().to(DEVICE), train_labels.to(device=DEVICE, dtype=torch.int64)\n",
    "        pseudo_inputs, pseudo_labels = pseudo_inputs.float().to(DEVICE), pseudo_labels.to(device=DEVICE, dtype=torch.int64) \n",
    "        #print(\"**\") \n",
    "        \n",
    "    \n",
    "        # Initialize gradients to zero\n",
    "        optimizer.zero_grad()  \n",
    "        # Feed-forward input data through the network\n",
    "        #print(\"***\")\n",
    "        train_outputs = model(train_inputs)\n",
    "        \n",
    "        #print(f'train_outputs: {torch.unique(train_outputs)}, train_labels: {torch.unique(train_labels)}')\n",
    "        if alpha > 0: # alpha>0이면 pseudo label 포함해서 loss 계산\n",
    "            pseudo_outputs = model(pseudo_inputs)\n",
    "            _, pseudo_labels = torch.max(pseudo_outputs.detach(), 1)     \n",
    "            loss = criterion(train_outputs, train_labels)  + alpha*criterion(pseudo_outputs, pseudo_labels)\n",
    "            #print(\"****\")\n",
    "        else:\n",
    "            loss = criterion(train_outputs, train_labels)\n",
    "        # print(next(model.parameters()).device)\n",
    "        # Backpropagate loss and compute gradients\n",
    "        loss.backward()\n",
    "        # Update the network parameters\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        #print(\"****\")\n",
    "\n",
    "        _, predicted = torch.max(train_outputs.detach(), 1)\n",
    "        total += train_labels.size(0)\n",
    "        correct += (predicted == train_labels).sum().item()\n",
    "\n",
    "        #print(\"*****\")\n",
    "\n",
    "    val_acc = accuracy(val_loader)\n",
    "    if val_acc >= best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save(model.state_dict(), './[BCSS]segmentation_model/sdm_semi_unet_resnest50.pth')    \n",
    "        print('[%d] train acc: %.2f, validation acc: %.2f - Saved the best model' %(epoch, 100*correct/total, val_acc))  \n",
    "        \n",
    "        print(\"*******\")\n",
    "    elif epoch % 10 == 0:\n",
    "        print('[%d] train acc: %.2f, validation acc: %.2f' %(epoch, 100*correct/total, val_acc))\n",
    "        \n",
    "        The \n",
    "\n",
    "        print(\"*******\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51c55b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pseudo-label training: with SDM\n",
    "torch.backends.cudnn.benchmark = False\n",
    "for epoch in range(iterations):\n",
    "    print(epoch,\"epoch start !\")\n",
    "    best_score=999999\n",
    "    epochStart = time.time()\n",
    "    runningLoss = 0\n",
    "    train_i_number = 0\n",
    "    val_i_number = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    best_acc = 0\n",
    "    model.train() # For training\n",
    "    print(\"train_loader\", len(train_loader))\n",
    "    print(\"unlabel_loader\", len(unlabel_loader))\n",
    "    for traindata, pseudodata in zip(train_loader, unlabel_loader):\n",
    "    # for data in tqdm.notebook.tqdm(train_loader):\n",
    "        #print(f\"train data {i+1}/{len(train_loader)}\")\n",
    "        #print(\"*\")\n",
    "        train_inputs = traindata['img']['image']\n",
    "        train_labels = traindata['label']\n",
    "\n",
    "        pseudo_inputs = pseudodata['img']['image']\n",
    "        pseudo_labels = pseudodata['label']\n",
    "        #print(\"train_label : \", type(train_inputs), \"pseudo label : \", type(pseudo_inputs))\n",
    "        \n",
    "        train_inputs, train_labels = train_inputs.float().to(DEVICE), train_labels.to(device=DEVICE, dtype=torch.int64)\n",
    "        pseudo_inputs, pseudo_labels = pseudo_inputs.float().to(DEVICE), pseudo_labels.to(device=DEVICE, dtype=torch.int64)\n",
    "        #print(\"**\") \n",
    "        \n",
    "        \n",
    "        # Initialize gradients to zero\n",
    "        optimizer.zero_grad()  \n",
    "        # Feed-forward input data through the network\n",
    "        #print(\"***\")\n",
    "        pred = model(train_inputs)\n",
    "        train_outputs = F.softmax(pred, dim=1)\n",
    "        print(f\"pred: {pred[0]}\")\n",
    "        print(f\"target shape: {train_labels[0]}\")\n",
    "        #print(\"pred:\", pred)\n",
    "\n",
    "        # sdm loss\n",
    "        #pred = torch.log(train_outputs)\n",
    "        #pred = pred - pred.mean()\n",
    "        pred_sdm = torch.tanh(pred)\n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            label_sdm = compute_sdf(train_labels.cpu().numpy(), pred_sdm.shape)\n",
    "            label_sdm = torch.from_numpy(label_sdm).float().to(device=DEVICE)\n",
    "        sdm_loss = F.l2_loss(pred_sdm, label_sdm) #l1_loss(pred_tanh, pred_sdm) #mse_loss()\n",
    "        # print(\"pred_tanh:\", pred_tanh, \"pred_sdm:\", pred_sdm, \"train output:\", train_outputs)\n",
    "        #print(\"*****\")\n",
    "\n",
    "        #print(\"**\")\n",
    "        if alpha > 0: # alpha>0이면 pseudo label 포함해서 loss 계산\n",
    "            pseudo_pred = model(pseudo_inputs)\n",
    "            pseudo_outputs = F.softmax(pseudo_pred, dim=1)\n",
    "            _, pseudo_labels = torch.max(pseudo_outputs.detach(), 1)\n",
    "\n",
    "  \n",
    "            pseudo_sdm = torch.tanh(pseudo_pred)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pseudo_label_sdm = compute_sdf(pseudo_labels.cpu().numpy(), pseudo_sdm.shape)\n",
    "                pseudo_label_sdm = torch.from_numpy(pseudo_label_sdm).float().to(device=DEVICE)\n",
    "            pseudo_sdm_loss = F.l2_loss(pseudo_sdm, pseudo_label_sdm) #l1_loss(pseudo_tanh, pseudo_sdm) #mse_loss\n",
    "            loss = criterion(train_outputs, train_labels)  + alpha*criterion(pseudo_outputs, pseudo_labels) + beta*(sdm_loss+pseudo_sdm_loss)\n",
    "            #print(\"sdm loss: %.2f, pseudo sdm loss: %.2f, sdm included loss: %.2f\" %(sdm_loss, pseudo_sdm_loss, loss))\n",
    "            #print(\"******\")\n",
    "        else:\n",
    "            loss = criterion(train_outputs, train_labels) + beta*sdm_loss\n",
    "            #print(\"sdm loss: %.2f, sdm included loss: %.2f \"%(sdm_loss, loss))\n",
    "            # print(\"loss: \", loss)\n",
    "            #print(\"******\") \n",
    "            \n",
    "        # Backpropagate loss and compute gradients\n",
    "        loss.backward()\n",
    "        # Update the network parameters\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        #print(\"****\")\n",
    "\n",
    "        _, predicted = torch.max(train_outputs.detach(), 1)\n",
    "        total += train_labels.size(0)\n",
    "        correct += (predicted == train_labels).sum().item()\n",
    "\n",
    "        #print(\"*****\")\n",
    "    \n",
    "        if (epoch > T1) and (epoch < T2):  #epoch이 5부터 70까지일 때 \n",
    "            alpha = alpha_t*(epoch - T1)/(T2 - T1)               \n",
    "            #print(\"******\")\n",
    "\n",
    "        elif epoch >= T2:    #epoch이 70이상일 때 \n",
    "            alpha = alpha_t\n",
    "            #print(\"******\")\n",
    "    val_acc = accuracy(val_loader)\n",
    "    if val_acc >= best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save(model.state_dict(), './model/resnest_pseudo5.pth')    \n",
    "        print('[%d] train acc: %.2f, validation acc: %.2f - Saved the best model' %(epoch, 100*correct/total, val_acc))  \n",
    "        wandb.log({\"train acc\": 100*correct/total, \"validation acc\":val_acc})\n",
    "        print(\"*******\")\n",
    "    elif epoch % 10 == 0:\n",
    "        print('[%d] train acc: %.2f, validation acc: %.2f' %(epoch, 100*correct/total, val_acc))\n",
    "        wandb.log({\"train acc\": 100*correct/total, \"validation acc\":val_acc})\n",
    "\n",
    "        print(\"*******\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3023b559",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b84ba6",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b285b44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "micro_iou_score_list = []\n",
    "micro_f1_score_list = []\n",
    "micro_accuracy_list = []\n",
    "micro_recall_list = []\n",
    "micro_precision_list = []\n",
    "micro_sensitivity_list = []\n",
    "micro_specificity_list = []\n",
    "\n",
    "macro_iou_score_list = []\n",
    "macro_f1_score_list = []\n",
    "macro_accuracy_list = []\n",
    "macro_recall_list = []\n",
    "macro_precision_list = []\n",
    "macro_sensitivity_list = []\n",
    "macro_specificity_list = []\n",
    "\n",
    "micro_imagewise_iou_score_list = []\n",
    "micro_imagewise_f1_score_list = []\n",
    "micro_imagewise_accuracy_list = []\n",
    "micro_imagewise_recall_list = []\n",
    "micro_imagewise_precision_list = []\n",
    "micro_imagewise_sensitivity_list = []\n",
    "micro_imagewise_specificity_list = []\n",
    "\n",
    "macro_imagewise_iou_score_list = []\n",
    "macro_imagewise_f1_score_list = []\n",
    "macro_imagewise_accuracy_list = []\n",
    "macro_imagewise_recall_list = []\n",
    "macro_imagewise_precision_list = []\n",
    "macro_imagewise_sensitivity_list = []\n",
    "macro_imagewise_specificity_list = []\n",
    "\n",
    "best_model=model\n",
    "n=0\n",
    "best_model.load_state_dict(torch.load('./model/resnest_sdm.pth', map_location=DEVICE))\n",
    "with torch.no_grad(): # torch.no_grad()를 하면 gradient 계산을 수행 안 함\n",
    "    best_model.eval()\n",
    "    total=len(test_loader)\n",
    "    for i, data in enumerate(itertools.islice(test_loader, total-1)):\n",
    "    # for i, data in enumerate(test_loader):\n",
    "    # for data in tqdm.notebook.tqdm(test_loader):\n",
    "        inputs = data['img']['image']\n",
    "        # print(f\"{i}: {inputs.shape}\")\n",
    "        #print(inputs)\n",
    "        labels = data['label'] \n",
    "\n",
    "        inputs, labels = inputs.float().to(DEVICE), labels.float().to(device=DEVICE, dtype=torch.int64) #inputs.float().to(DEVICE), labels.float().to(device=DEVICE, dtype=torch.int64)    \n",
    "        preds = best_model(inputs)    \n",
    "        output = F.softmax(preds, dim=1)\n",
    " \n",
    "        target = labels\n",
    "\n",
    "\n",
    "        _, output = torch.max(output, 1)\n",
    "\n",
    "        tp, fp, fn, tn = smp.metrics.get_stats(output, target, mode='multilabel', threshold=0.5)        \n",
    "        \n",
    "        # then compute metrics with required reduction (see metric docs)\n",
    "        # micro, macro, weighted,\n",
    "        \n",
    "        micro_iou_score = round(smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\").item(), 3)\n",
    "        micro_f1_score = round(smp.metrics.f1_score(tp, fp, fn, tn, reduction=\"micro\").item(), 3)\n",
    "        micro_accuracy = round(smp.metrics.accuracy(tp, fp, fn, tn, reduction=\"micro\").item(),3)\n",
    "        micro_recall = round(smp.metrics.recall(tp, fp, fn, tn, reduction=\"micro\").item(), 3)\n",
    "        micro_precision = round(smp.metrics.precision(tp, fp, fn, tn, reduction=\"micro\").item(), 3)\n",
    "        micro_sensitivity = round(smp.metrics.sensitivity(tp, fp, fn, tn, reduction=\"micro\").item(), 3)\n",
    "        micro_specificity = round(smp.metrics.specificity(tp, fp, fn, tn, reduction=\"micro\").item(), 3)\n",
    "\n",
    "        macro_iou_score = round(smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"macro\").item(), 3)\n",
    "        macro_f1_score = round(smp.metrics.f1_score(tp, fp, fn, tn, reduction=\"macro\").item(), 3)\n",
    "        macro_accuracy = round(smp.metrics.accuracy(tp, fp, fn, tn, reduction=\"macro\").item(),3)\n",
    "        macro_recall = round(smp.metrics.recall(tp, fp, fn, tn, reduction=\"macro\").item(), 3)\n",
    "        macro_precision = round(smp.metrics.precision(tp, fp, fn, tn, reduction=\"macro\").item(), 3)\n",
    "        macro_sensitivity = round(smp.metrics.sensitivity(tp, fp, fn, tn, reduction=\"macro\").item(), 3)\n",
    "        macro_specificity = round(smp.metrics.specificity(tp, fp, fn, tn, reduction=\"macro\").item(), 3)\n",
    "\n",
    "        micro_imagewise_iou_score = round(smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro-imagewise\").item(), 3)\n",
    "        micro_imagewise_f1_score = round(smp.metrics.f1_score(tp, fp, fn, tn, reduction=\"micro-imagewise\").item(), 3)\n",
    "        micro_imagewise_accuracy = round(smp.metrics.accuracy(tp, fp, fn, tn, reduction=\"micro-imagewise\").item(),3)\n",
    "        micro_imagewise_recall = round(smp.metrics.recall(tp, fp, fn, tn, reduction=\"micro-imagewise\").item(), 3)\n",
    "        micro_imagewise_precision = round(smp.metrics.precision(tp, fp, fn, tn, reduction=\"micro-imagewise\").item(), 3)\n",
    "        micro_imagewise_sensitivity = round(smp.metrics.sensitivity(tp, fp, fn, tn, reduction=\"micro-imagewise\").item(), 3)\n",
    "        micro_imagewise_specificity = round(smp.metrics.specificity(tp, fp, fn, tn, reduction=\"micro-imagewise\").item(), 3)\n",
    "\n",
    "        macro_imagewise_iou_score = round(smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"macro-imagewise\").item(), 3)\n",
    "        macro_imagewise_f1_score = round(smp.metrics.f1_score(tp, fp, fn, tn, reduction=\"macro-imagewise\").item(), 3)\n",
    "        macro_imagewise_accuracy = round(smp.metrics.accuracy(tp, fp, fn, tn, reduction=\"macro-imagewise\").item(),3)\n",
    "        macro_imagewise_recall = round(smp.metrics.recall(tp, fp, fn, tn, reduction=\"macro-imagewise\").item(), 3)\n",
    "        macro_imagewise_precision = round(smp.metrics.precision(tp, fp, fn, tn, reduction=\"macro-imagewise\").item(), 3)\n",
    "        macro_imagewise_sensitivity = round(smp.metrics.sensitivity(tp, fp, fn, tn, reduction=\"macro-imagewise\").item(), 3)\n",
    "        macro_imagewise_specificity = round(smp.metrics.specificity(tp, fp, fn, tn, reduction=\"macro-imagewise\").item(), 3)\n",
    "\n",
    "\n",
    "        micro_iou_score_list.append(micro_iou_score)\n",
    "        micro_f1_score_list.append(micro_f1_score)\n",
    "        micro_accuracy_list.append(micro_accuracy)\n",
    "        micro_recall_list.append(micro_recall)\n",
    "        micro_precision_list.append(micro_precision)\n",
    "        micro_sensitivity_list.append(micro_sensitivity)\n",
    "        micro_specificity_list.append(micro_specificity)\n",
    "\n",
    "        macro_iou_score_list.append(macro_iou_score)\n",
    "        macro_f1_score_list.append(macro_f1_score)\n",
    "        macro_accuracy_list.append(macro_accuracy)\n",
    "        macro_recall_list.append(macro_recall)\n",
    "        macro_precision_list.append(macro_precision)\n",
    "        macro_sensitivity_list.append(macro_sensitivity)\n",
    "        macro_specificity_list.append(macro_specificity)\n",
    "\n",
    "        micro_imagewise_iou_score_list.append(micro_imagewise_iou_score)\n",
    "        micro_imagewise_f1_score_list.append(micro_imagewise_f1_score)\n",
    "        micro_imagewise_accuracy_list.append(micro_imagewise_accuracy)\n",
    "        micro_imagewise_recall_list.append(micro_imagewise_recall)\n",
    "        micro_imagewise_precision_list.append(micro_imagewise_precision)\n",
    "        micro_imagewise_sensitivity_list.append(micro_imagewise_sensitivity)\n",
    "        micro_imagewise_specificity_list.append(micro_imagewise_specificity)\n",
    "\n",
    "        macro_imagewise_iou_score_list.append(macro_imagewise_iou_score)\n",
    "        macro_imagewise_f1_score_list.append( macro_imagewise_f1_score)\n",
    "        macro_imagewise_accuracy_list.append(macro_imagewise_accuracy)\n",
    "        macro_imagewise_recall_list.append(macro_imagewise_recall)\n",
    "        macro_imagewise_precision_list.append(macro_imagewise_precision)\n",
    "        macro_imagewise_sensitivity_list.append(macro_imagewise_sensitivity)\n",
    "        macro_imagewise_specificity_list.append(macro_imagewise_specificity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dec83f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,3))\n",
    "plt.title('micro_iou_score')\n",
    "plt.plot(micro_iou_score_list,'r-',label='IoU_score')\n",
    "plt.grid(color = 'gray', linestyle = ':', linewidth = 0.5)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(5,3))\n",
    "plt.title('micro_f1_score')\n",
    "plt.plot(micro_f1_score_list,'b-',label='f1_score')\n",
    "plt.grid(color = 'gray', linestyle = ':', linewidth = 0.5)\n",
    "\n",
    "fig = plt.figure(figsize=(5,3))\n",
    "plt.title('micro_accuracy')\n",
    "plt.plot(micro_accuracy_list,'y-',label='accuracy')\n",
    "plt.grid(color = 'gray', linestyle = ':', linewidth = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55ffa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def metrics_average(metrics_list):\n",
    "    metric_avg = sum([x for x in metrics_list if isinstance(x, (int, float)) and not math.isnan(x)])/len(metrics_list)\n",
    "    return round(metric_avg, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b433fe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"micro_iou_score: \",metrics_average(micro_iou_score_list))\n",
    "print(\"micro_f1_score_score: \",metrics_average(micro_f1_score_list))\n",
    "print(\"micro_accuracy_score: \",metrics_average(micro_accuracy_list))\n",
    "print(\"micro_recall_score: \",metrics_average(micro_recall_list))\n",
    "print(\"micro_precision_score: \",metrics_average(micro_precision_list))\n",
    "print(\"micro_sensitivity_score: \",metrics_average(micro_sensitivity_list))\n",
    "print(\"micro_specificity_score: \",metrics_average(micro_specificity_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b552bf76",
   "metadata": {},
   "source": [
    "### visualize at once(input, target, prediction, sdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2dc729",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6df128",
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_classes = ['background', 'tumor', 'stroma', 'dcis']\n",
    "sem_class_to_idx = {cls: idx for (idx, cls) in enumerate(sem_classes)} \n",
    "tumor_category = sem_class_to_idx[\"tumor\"]\n",
    "stroma_category = sem_class_to_idx[\"stroma\"]\n",
    "dcis_category = sem_class_to_idx[\"dcis\"]\n",
    "\n",
    "\n",
    "#x,y존재\n",
    "best_model = model\n",
    "best_model.load_state_dict(torch.load('./[BCSS]segmentation_model/sdm_semi_unet_resnest50.pth', map_location=DEVICE))\n",
    "def visualize_one_sample(test_x_data, test_y_data, model):\n",
    "    \"\"\"\n",
    "    하나의 샘플에 대해 Input, Target, Prediction, SDM Output을 시각화합니다.\n",
    "    \n",
    "    Args:\n",
    "        test_x_data (torch.Tensor): 입력 이미지, shape=(batch, C, H, W)\n",
    "        test_y_data (torch.Tensor): 타겟 마스크, shape=(batch, H, W) 또는 (batch, 1, H, W)\n",
    "        model (torch.nn.Module): 학습된 segmentation 모델\n",
    "    \"\"\"\n",
    "    model.eval() \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        X = test_x_data.float().to(DEVICE)\n",
    "        Y = test_y_data\n",
    "        prediction = model(X)  # 예: output shape=(B, num_classes, H, W)\n",
    "        #sdm\n",
    "        prediction = F.softmax(prediction, dim=1)\n",
    "        \n",
    "      \n",
    "        _, pred_mask = torch.max(prediction, 1)  # shape=(B, H, W)\n",
    "        pred_mask = pred_mask[0].cpu().numpy()\n",
    "        target_mask = Y[0].cpu().numpy().squeeze()\n",
    "\n",
    "       \n",
    "        input_img = X[0].cpu().numpy()\n",
    "        if input_img.ndim == 3 and input_img.shape[0] in [1, 3]:\n",
    "            if input_img.shape[0] == 1:\n",
    "                input_img = input_img.squeeze(0)\n",
    "                cmap_input = 'viridis' \n",
    "            else:\n",
    "                input_img = np.transpose(input_img, (1, 2, 0))\n",
    "                cmap_input = None \n",
    "        else:\n",
    "            cmap_input = 'viridis'\n",
    "        \n",
    "      \n",
    "        # binary_mask = ((pred_mask == tumor_category) | (pred_mask == stroma_category)).astype(np.uint8)\n",
    "        binary_mask = np.where(np.isin(pred_mask, [0, 1, 2]), pred_mask, 0)\n",
    "\n",
    "        # sdm = compute_sdm(pred_mask).squeeze()\n",
    "        # sdm = torch.from_numpy(sdm).float()\n",
    "        \n",
    "\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    axs[0].imshow(input_img, cmap=cmap_input)\n",
    "    axs[0].set_title(\"Input\")\n",
    "    axs[0].axis(\"off\")\n",
    "    \n",
    "    \n",
    "    axs[1].imshow(target_mask, cmap='viridis') # bianry_mask\n",
    "    axs[1].set_title(\"Target\")\n",
    "    axs[1].axis(\"off\")\n",
    "    \n",
    "\n",
    "    axs[2].imshow(pred_mask, cmap='viridis')\n",
    "    axs[2].set_title(\"Prediction\")\n",
    "    axs[2].axis(\"off\")\n",
    "    \n",
    "    # im = axs[3].imshow(sdm, cmap='seismic')\n",
    "    # axs[3].set_title(\"SDM Output\")\n",
    "    # axs[3].axis(\"off\")\n",
    "    # fig.colorbar(im, ax=axs[3], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6df128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.io import read_image\n",
    "\n",
    "\n",
    "test_x_data_path = ''\n",
    "test_y_data_path = ''\n",
    "\n",
    "test_x_data = sorted(os.listdir(test_x_data_path))\n",
    "test_y_data = sorted(os.listdir(test_y_data_path))\n",
    "\n",
    "for i in range(50, 52):\n",
    "\n",
    "   xname = test_x_data[i]\n",
    "   yname = test_y_data[i]\n",
    "\n",
    "   x_path = os.path.join(test_x_data_path, xname)\n",
    "   y_path = os.path.join(test_y_data_path, yname)\n",
    "    \n",
    "    \n",
    "   x = cv2.imread(x_path)\n",
    "   x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "    # plt.imshow(x)\n",
    "   X = torch.Tensor(x)\n",
    "   X = X /255\n",
    "    \n",
    "    # print(X)\n",
    "     \n",
    "   y = cv2.imread(y_path)    \n",
    "   y = cv2.cvtColor(y, cv2.COLOR_BGR2RGB)\n",
    "    # plt.imshow(y)\n",
    "   Y = torch.Tensor(y)\n",
    "    # print(Y)\n",
    "    \n",
    "    #y = cv2.imread(yname)\n",
    "    #y = cv2.cvtColor(cv2.imread(yname), cv2.COLOR_BGR2RGB)\n",
    "    #Y = TF.to_tensor(y)\n",
    "    \n",
    "\n",
    "   print(\"x file : \",xname)\n",
    "   print(\"y file : \",yname)\n",
    "\n",
    "    \n",
    "   X = np.transpose(X, (2, 0, 1))\n",
    "   Y = np.transpose(Y, (2, 0, 1))\n",
    "    \n",
    "   X.unsqueeze_(0)\n",
    "   Y.unsqueeze_(0)\n",
    "    \n",
    "   #데이터 shape 출력\n",
    "   # print(\"기본 x shape : \",X.shape)\n",
    "   # print(\"기본 y shape : \",X.shape)\n",
    "   # \n",
    "    \n",
    "   # 시각화: 한 행에 2개의 이미지 (좌측: x, 우측: y)\n",
    "   # plt.subplot(1, 2, 1)\n",
    "   # plt.imshow(x)\n",
    "   # plt.title(\"X Image\")\n",
    "\n",
    "   # plt.subplot(1, 2, 2)\n",
    "   # plt.imshow(y)\n",
    "   # plt.title(\"Y Mask\")\n",
    "\n",
    "   # plt.show()\n",
    "\n",
    "   visualize_one_sample(X, Y, best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf12182b",
   "metadata": {},
   "source": [
    "### calculate FLOPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccf1110",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptflops import get_model_complexity_info\n",
    "macs, params = get_model_complexity_info(model, input_res=(3, 256, 256), as_strings=True)\n",
    "\n",
    "print(f'FLOPs: {macs}, Params: {params}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pseudo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
